{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arlington Project 2050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arlington Project 2050 Background\n",
    "Arlington County wanted to ask the people of their county what they thought and wanted Arlington to look like in the year 2050. The county asked people to fill out postcards on this topic, these postcards came from many diffrent colection points from events like the county fair and the Hispanc heratige community festival to online submission. \n",
    "\n",
    "### Data Set: Public Input\n",
    "The data set I focused on is called Public Input and it is postcards that were filled out online. Each persons postcard(s) (labeled as comment in data set) has a comment ID, an generated zip code of the person writting the postcard(s), where the post card is from (all are from online sources/'web' in this data set). Some rows have multiple postcards including a postcard about steps to get to the ideal future and some rows may only have one postcard/comment. The last column is the users zip code manually typed in which not all rows have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for this project\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "from sklearn.decomposition import PCA  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by just uploading the excel file into pandas and printing it so I could see what needed to be cleaned. I noticed that there was many NaN values throughout the data. I also noticed that one of the columns of the dataset was about steps to getting to the futre which is dffrent than the rest of the comments. I choose to keep that comment and add it to the column called concatenate_text. In the column all of the comments including the one about the steps to get to the future are put together if they are in the same row and all of the NaN values are taken out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "public_input = pd.read_excel(\"Public Input Potcards Exported 10-01-24.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the columns\n",
    "public_input.columns = ['id', 'zip', 'source', 'first', 'first_gettinghere','second','third','zip_selfreported','zip_selfreported2']\n",
    "\n",
    "# Create a single column that concatenates the text from the other columns\n",
    "def concatenate_text(row):\n",
    "    text_ = \"\"\n",
    "    if pd.notna(row['first']):\n",
    "        text_ += row['first']\n",
    "    if pd.notna(row['first_gettinghere']):\n",
    "    \n",
    "        text_ += row['first_gettinghere']     \n",
    "    if pd.notna(row['second']):\n",
    "        text_ += row['second']\n",
    "    if pd.notna(row['third']):\n",
    "        text_ += row['third']\n",
    "\n",
    "    return text_\n",
    "\n",
    "public_input['concatenated_text'] = public_input.apply(concatenate_text, axis=1)\n",
    "public_input['source'] = \"web\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "\n",
    "After cleaning the data the first thing I did was visualize the data by making a word cloud. I made a varaible that has a list of the text in the concactenated_text as a string to pass through the generate function at the end of the word cloud. I tried to use the column itself but it wasn't the correct data type. The rest of the code is just to put a peramiters on the word cloud. Using the concatenated text was helpful because without that data cleaning there would be NaN showing up in the word cloud as a word when really it is just a placeholder that labels the abscence of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(public_input['concatenated_text'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, mask=None, contour_width=3, contour_color='steelblue').generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity \n",
    "For polarity I used a for loop that goes to every item in the column and then creates the vector for the item and calcuates the polarity and subjectivity. This creates an new column in the data set that has the polairty and subjectivity of each row of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "#Example 1\n",
    "\n",
    "for i in range (len(public_input['concatenated_text'])):\n",
    "    doc = nlp(public_input.loc[i]['concatenated_text'])\n",
    "    public_input.loc[i, 'polarity'] = doc._.blob.polarity\n",
    "    public_input.loc[i, 'subjectivity'] = doc._.blob.subjectivity \n",
    "\n",
    "public_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embedding \n",
    "I couldn't figure out the dimesnonality reduction and scatterplot but I was able to find the vector for the whole column of concatenated text and each element in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in public_input['concatenated_text']:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        vector = token.vector_norm\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    vector = token.vector_norm\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "I did research on K-Means Clustering but I wasn't able to apply it but I wonder what the clusters would look like, they would hopefully all containt the same subject per cluster but the subjectivty and polairty of each cluster would be intresting and even maybe a word cloud for the indvidual clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "This project was a great oppertunity and I feel like I learned more about the county and the real life applications of pandas. I have learned about how to do natural language proccessing and about diffrent "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
